{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall pyspark\n",
    "\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "     ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "     - -------------------------------------- 20.5/626.3 kB ? eta -:--:--\n",
      "     --- --------------------------------- 51.2/626.3 kB 871.5 kB/s eta 0:00:01\n",
      "     ---- -------------------------------- 81.9/626.3 kB 762.6 kB/s eta 0:00:01\n",
      "     ------ ----------------------------- 112.6/626.3 kB 819.2 kB/s eta 0:00:01\n",
      "     -------- --------------------------- 153.6/626.3 kB 833.5 kB/s eta 0:00:01\n",
      "     ---------- ------------------------- 184.3/626.3 kB 794.9 kB/s eta 0:00:01\n",
      "     ------------ ----------------------- 215.0/626.3 kB 769.9 kB/s eta 0:00:01\n",
      "     -------------- --------------------- 245.8/626.3 kB 752.5 kB/s eta 0:00:01\n",
      "     --------------- -------------------- 276.5/626.3 kB 774.0 kB/s eta 0:00:01\n",
      "     ------------------ ----------------- 317.4/626.3 kB 785.7 kB/s eta 0:00:01\n",
      "     -------------------- --------------- 348.2/626.3 kB 771.5 kB/s eta 0:00:01\n",
      "     --------------------- -------------- 378.9/626.3 kB 761.7 kB/s eta 0:00:01\n",
      "     ----------------------- ------------ 409.6/626.3 kB 752.1 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 430.1/626.3 kB 767.5 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 460.8/626.3 kB 758.5 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 491.5/626.3 kB 750.8 kB/s eta 0:00:01\n",
      "     ------------------------------ ----- 522.2/626.3 kB 744.2 kB/s eta 0:00:01\n",
      "     -------------------------------- --- 563.2/626.3 kB 753.4 kB/s eta 0:00:01\n",
      "     ---------------------------------- - 593.9/626.3 kB 762.0 kB/s eta 0:00:01\n",
      "     ------------------------------------ 626.3/626.3 kB 744.1 kB/s eta 0:00:00\n",
      "Collecting nltk>=3.8\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.0/1.5 MB 653.6 kB/s eta 0:00:03\n",
      "     -- ------------------------------------- 0.1/1.5 MB 762.6 kB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.1/1.5 MB 819.2 kB/s eta 0:00:02\n",
      "     --- ------------------------------------ 0.1/1.5 MB 774.0 kB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.2/1.5 MB 748.1 kB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.2/1.5 MB 731.4 kB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.2/1.5 MB 765.3 kB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.3/1.5 MB 744.2 kB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.3/1.5 MB 737.3 kB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.3/1.5 MB 752.2 kB/s eta 0:00:02\n",
      "     --------- ------------------------------ 0.4/1.5 MB 742.3 kB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.4/1.5 MB 734.3 kB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.4/1.5 MB 727.5 kB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.5/1.5 MB 723.0 kB/s eta 0:00:02\n",
      "     ------------- -------------------------- 0.5/1.5 MB 731.4 kB/s eta 0:00:02\n",
      "     -------------- ------------------------- 0.5/1.5 MB 725.4 kB/s eta 0:00:02\n",
      "     --------------- ------------------------ 0.6/1.5 MB 734.2 kB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.6/1.5 MB 729.7 kB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 0.6/1.5 MB 739.1 kB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 0.7/1.5 MB 734.7 kB/s eta 0:00:02\n",
      "     ------------------- -------------------- 0.7/1.5 MB 729.2 kB/s eta 0:00:02\n",
      "     -------------------- ------------------- 0.8/1.5 MB 735.7 kB/s eta 0:00:02\n",
      "     -------------------- ------------------- 0.8/1.5 MB 732.1 kB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 0.8/1.5 MB 738.7 kB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 0.9/1.5 MB 735.2 kB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.9/1.5 MB 732.0 kB/s eta 0:00:01\n",
      "     ------------------------ --------------- 0.9/1.5 MB 739.2 kB/s eta 0:00:01\n",
      "     ------------------------- -------------- 0.9/1.5 MB 727.4 kB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.0/1.5 MB 726.4 kB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.0/1.5 MB 726.4 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 699.2 kB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.0/1.5 MB 705.0 kB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.0/1.5 MB 704.0 kB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.1/1.5 MB 702.5 kB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.1/1.5 MB 701.0 kB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.1/1.5 MB 706.4 kB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.2/1.5 MB 704.9 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.2/1.5 MB 709.8 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.2/1.5 MB 708.6 kB/s eta 0:00:01\n",
      "     --------------------------------- ------ 1.3/1.5 MB 707.2 kB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.3/1.5 MB 706.0 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.3/1.5 MB 710.0 kB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.4/1.5 MB 708.6 kB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.4/1.5 MB 707.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.4/1.5 MB 711.7 kB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 715.3 kB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 714.0 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 698.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.9.11-cp39-cp39-win_amd64.whl (274 kB)\n",
      "     ---------------------------------------- 0.0/274.1 kB ? eta -:--:--\n",
      "     ---- -------------------------------- 30.7/274.1 kB 660.6 kB/s eta 0:00:01\n",
      "     -------- ---------------------------- 61.4/274.1 kB 656.4 kB/s eta 0:00:01\n",
      "     ------------ ------------------------ 92.2/274.1 kB 751.6 kB/s eta 0:00:01\n",
      "     -------------- --------------------- 112.6/274.1 kB 726.2 kB/s eta 0:00:01\n",
      "     ------------------ ----------------- 143.4/274.1 kB 708.1 kB/s eta 0:00:01\n",
      "     ---------------------- ------------- 174.1/274.1 kB 748.1 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 204.8/274.1 kB 731.4 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 256.0/274.1 kB 714.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ 274.1/274.1 kB 703.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "Requirement already satisfied: click in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Installing collected packages: regex, nltk, textblob\n",
      "Successfully installed nltk-3.9.1 regex-2024.9.11 textblob-0.18.0.post0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dash in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (2.18.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (5.14.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: dash-table==5.0.0 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from dash) (5.0.0)\n",
      "Requirement already satisfied: dash-html-components==2.0.0 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from dash) (2.0.0)\n",
      "Requirement already satisfied: Flask<3.1,>=1.0.4 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from dash) (3.0.3)\n",
      "Requirement already satisfied: Werkzeug<3.1 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from dash) (3.0.3)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from dash) (1.5.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from dash) (65.6.3)\n",
      "Requirement already satisfied: retrying in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from dash) (1.3.4)\n",
      "Requirement already satisfied: dash-core-components==2.0.0 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from dash) (2.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from dash) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from dash) (4.12.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from dash) (4.11.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from plotly) (8.2.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from plotly) (22.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash) (1.6.2)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash) (8.1.7)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash) (3.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from importlib-metadata->dash) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from Werkzeug<3.1->dash) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from requests->dash) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from requests->dash) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from requests->dash) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from requests->dash) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhay\\anaconda3\\envs\\eda\\lib\\site-packages (from click>=8.1.3->Flask<3.1,>=1.0.4->dash) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install dash plotly pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType,\n",
    "    IntegerType, TimestampType\n",
    ")\n",
    "from pyspark.sql.functions import col, to_timestamp, udf, count, avg\n",
    "from pyspark.sql import functions as F\n",
    "import datetime\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CustomerFeedbackAnalysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for feedback data\n",
    "feedback_schema = StructType([\n",
    "    StructField(\"Customer_ID\", StringType(), False),\n",
    "    StructField(\"Feedback_Channel\", StringType(), False),\n",
    "    StructField(\"Rating\", IntegerType(), False),\n",
    "    StructField(\"Comment\", StringType(), True),\n",
    "    StructField(\"Date\", TimestampType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"DataIngestion\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o113.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (LAPTOP-SDEV92GG executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Generate the Spark DataFrame\u001b[39;00m\n\u001b[0;32m     22\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m generate_spark_data(\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Abhay\\anaconda3\\envs\\EDA\\lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Abhay\\anaconda3\\envs\\EDA\\lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Abhay\\anaconda3\\envs\\EDA\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Abhay\\anaconda3\\envs\\EDA\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Abhay\\anaconda3\\envs\\EDA\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o113.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (LAPTOP-SDEV92GG executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "def generate_spark_data(num_records=1000):\n",
    "    \"\"\"Generate sample data in Spark\"\"\"\n",
    "    channels = [\"Email\", \"Social Media\", \"Survey\",\"Live chat\",\"Interview\"]\n",
    "    comments = [\"Too many bugs in the app....\", \"Too many ads on the platform....\", \"Love the new updates!\", \"Fantastic i love it !\", \"Horrible service to slow!\", \"Will be recommend !\",\"I faced issues with payment.\",\"Product is overpriced.\",\"I received wrong item.\"]\n",
    "\n",
    "    data = []\n",
    "    current_date = datetime.datetime.now()\n",
    "\n",
    "    for _ in range(num_records):\n",
    "        data.append((\n",
    "            f\"C{random.randint(10000, 99999)}\",\n",
    "            random.choice(channels),\n",
    "            random.randint(1, 5),\n",
    "            random.choice(comments),\n",
    "            (current_date - datetime.timedelta(days=random.randint(0, 30)))\n",
    "        ))\n",
    "\n",
    "    # Create Spark DataFrame\n",
    "    return spark.createDataFrame(data, [\"Customer_ID\", \"Feedback_Channel\", \"Rating\", \"Comment\", \"Date\"])\n",
    "\n",
    "# Generate the Spark DataFrame\n",
    "spark_df = generate_spark_data(1000)\n",
    "spark_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_to_pandas(spark_df):\n",
    "    \"\"\"Convert Spark DataFrame to Pandas DataFrame\"\"\"\n",
    "    try:\n",
    "        pandas_df = spark_df.toPandas()\n",
    "        print(f\"Successfully converted {len(pandas_df)} records to pandas DataFrame\")\n",
    "        return pandas_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting to pandas: {e}\")\n",
    "        return None\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = spark_to_pandas(spark_df.limit(1000))\n",
    "print(pandas_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Clean the feedback comments\"\"\"\n",
    "    # Remove unnecessary characters and handle missing values\n",
    "    df['Comment'] = df['Comment'].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True).str.strip()\n",
    "    df['Comment'] = df['Comment'].fillna('No Comment')\n",
    "    return df\n",
    "\n",
    "# Clean the data\n",
    "pandas_df = clean_data(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(comment):\n",
    "    \"\"\"Analyze sentiment of the comment\"\"\"\n",
    "    analysis = TextBlob(comment)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "spark_df = spark_df.withColumn(\"Sentiment\", sentiment_udf(col(\"Comment\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_analysis(pandas_df):\n",
    "    \"\"\"Perform basic analysis on the pandas DataFrame\"\"\"\n",
    "    if pandas_df is None:\n",
    "        print(\"No data available for analysis\")\n",
    "        return None\n",
    "\n",
    "    analysis = {\n",
    "        'total_records': len(pandas_df),\n",
    "        'channel_distribution': pandas_df['Feedback_Channel'].value_counts().to_dict(),\n",
    "        'average_rating': pandas_df['Rating'].mean(),\n",
    "        'rating_distribution': pandas_df['Rating'].value_counts().sort_index().to_dict()\n",
    "    }\n",
    "\n",
    "    return analysis\n",
    "\n",
    "# Perform analysis on the processed Pandas DataFrame\n",
    "analysis = perform_analysis(pandas_df)\n",
    "if analysis:\n",
    "    print(\"\\nAnalysis Results:\")\n",
    "    for key, value in analysis.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Trend analysis\n",
    "def trend_analysis(spark_df):\n",
    "    \"\"\"Analyze trends over time and by channel\"\"\"\n",
    "    # Convert date to date format\n",
    "    spark_df = spark_df.withColumn(\"Date\", to_timestamp(col(\"Date\")))\n",
    "\n",
    "    # Average ratings per channel\n",
    "    avg_rating_per_channel = spark_df.groupBy(\"Feedback_Channel\").agg(avg(\"Rating\").alias(\"Average_Rating\"))\n",
    "    avg_rating_per_channel.show()\n",
    "\n",
    "    # Count of sentiments by channel\n",
    "    sentiment_distribution = spark_df.groupBy(\"Feedback_Channel\", \"Sentiment\").agg(count(\"Sentiment\").alias(\"Count\"))\n",
    "    sentiment_distribution.show()\n",
    "\n",
    "    # Generate plots\n",
    "    return avg_rating_per_channel, sentiment_distribution\n",
    "\n",
    "avg_rating_per_channel, sentiment_distribution = trend_analysis(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualization Functions\n",
    "def visualize_sentiment_distribution(sentiment_distribution):\n",
    "    \"\"\"Visualize sentiment distribution\"\"\"\n",
    "    sentiment_pd = sentiment_distribution.toPandas()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Feedback_Channel', y='Count', hue='Sentiment', data=sentiment_pd)\n",
    "    plt.title('Sentiment Distribution by Feedback Channel')\n",
    "    plt.ylabel('Count of Sentiments')\n",
    "    plt.xlabel('Feedback Channel')\n",
    "    plt.legend(title='Sentiment')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_average_ratings(avg_rating_per_channel):\n",
    "    \"\"\"Visualize average ratings by feedback channel\"\"\"\n",
    "    avg_rating_pd = avg_rating_per_channel.toPandas()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Feedback_Channel', y='Average_Rating', data=avg_rating_pd)\n",
    "    plt.title('Average Ratings by Feedback Channel')\n",
    "    plt.ylabel('Average Rating')\n",
    "    plt.xlabel('Feedback Channel')\n",
    "    plt.ylim(0, 5)  # Set y-axis limit\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trends_over_time(spark_df):\n",
    "    \"\"\"Visualize feedback trends over time\"\"\"\n",
    "    trend_data = spark_df.groupBy(F.to_date(col(\"Date\")).alias(\"Date\")).agg(count(\"Customer_ID\").alias(\"Feedback_Count\"))\n",
    "    trend_data_pd = trend_data.toPandas()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=trend_data_pd, x='Date', y='Feedback_Count')\n",
    "    plt.title('Feedback Trends Over Time')\n",
    "    plt.ylabel('Number of Feedbacks')\n",
    "    plt.xlabel('Date')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_trends_over_time(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sentiment_distribution(sentiment_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_average_ratings(avg_rating_per_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_average_ratings_as_pie(avg_rating_per_channel):\n",
    "    \"\"\"Visualize average ratings by feedback channel as a pie chart with percentage contribution.\"\"\"\n",
    "    # Convert to Pandas DataFrame for plotting\n",
    "    avg_rating_pd = avg_rating_per_channel.toPandas()\n",
    "\n",
    "    # Calculate total average rating for scaling\n",
    "    avg_rating_pd['Percentage'] = (avg_rating_pd['Average_Rating'] / avg_rating_pd['Average_Rating'].sum()) * 100\n",
    "\n",
    "    # Create the pie chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.pie(avg_rating_pd['Percentage'], labels=avg_rating_pd['Feedback_Channel'], autopct='%1.1f%%', startangle=140)\n",
    "    plt.title('Average Ratings by Feedback Channel (Percentage)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_average_ratings_as_pie(avg_rating_per_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import col, to_timestamp, avg, count, udf\n",
    "from textblob import TextBlob\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# 1. Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CustomerFeedbackAnalysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Define schema for feedback data\n",
    "feedback_schema = StructType([\n",
    "    StructField(\"Customer_ID\", StringType(), False),\n",
    "    StructField(\"Feedback_Channel\", StringType(), False),\n",
    "    StructField(\"Rating\", IntegerType(), False),\n",
    "    StructField(\"Comment\", StringType(), True),\n",
    "    StructField(\"Date\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "def generate_spark_data(num_records=1000):\n",
    "    \"\"\"Generate sample data in Spark\"\"\"\n",
    "    channels = [\"Email\", \"Social Media\", \"Survey\", \"Live chat\", \"Interview\"]\n",
    "    comments = [\"Too many bugs in the app....\", \"Too many ads on the platform....\",\n",
    "                \"Love the new updates!\", \"Fantastic i love it !\", \"Horrible service to slow!\",\n",
    "                \"Will be recommend !\", \"I faced issues with payment.\", \"Product is overpriced.\",\n",
    "                \"I received wrong item.\"]\n",
    "    data = []\n",
    "    current_date = datetime.datetime.now()\n",
    "\n",
    "    for _ in range(num_records):\n",
    "        data.append((\n",
    "            f\"C{random.randint(10000, 99999)}\",\n",
    "            random.choice(channels),\n",
    "            random.randint(1, 5),\n",
    "            random.choice(comments),\n",
    "            (current_date - datetime.timedelta(days=random.randint(0, 30)))\n",
    "        ))\n",
    "\n",
    "    # Create Spark DataFrame\n",
    "    return spark.createDataFrame(data, [\"Customer_ID\", \"Feedback_Channel\", \"Rating\", \"Comment\", \"Date\"])\n",
    "\n",
    "# Generate the Spark DataFrame\n",
    "spark_df = generate_spark_data(1000)\n",
    "# 3. Define function for sentiment analysis\n",
    "def analyze_sentiment(comment):\n",
    "    \"\"\"Analyze sentiment of the comment\"\"\"\n",
    "    analysis = TextBlob(comment)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "spark_df = spark_df.withColumn(\"Sentiment\", sentiment_udf(col(\"Comment\")))\n",
    "\n",
    "# 6. Initialize the Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Layout of the Dashboard\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Customer Feedback Analysis Dashboard\", style={'text-align': 'center'}),\n",
    "\n",
    "    dcc.Tabs([\n",
    "        dcc.Tab(label='Sentiment Distribution by Channel', children=[\n",
    "            dcc.Graph(\n",
    "                id='sentiment-channel-bar',\n",
    "                figure=px.bar(\n",
    "                    sentiment_distribution_pd,\n",
    "                    x='Feedback_Channel',\n",
    "                    y='Count',\n",
    "                    color='Sentiment',\n",
    "                    title=\"Sentiment Distribution by Feedback Channel\",\n",
    "                    barmode='group',\n",
    "                    color_discrete_sequence=px.colors.qualitative.Pastel\n",
    "                )\n",
    "            )\n",
    "        ]),\n",
    "\n",
    "        dcc.Tab(label='Trend of Average Ratings Over Time', children=[\n",
    "            dcc.Graph(\n",
    "                id='avg-rating-trend-line',\n",
    "                figure=px.line(\n",
    "                    avg_rating_trend,\n",
    "                    x='Date',\n",
    "                    y='Rating',\n",
    "                    title='Trend of Average Ratings Over Time',\n",
    "                    markers=True,\n",
    "                    color_discrete_sequence=['royalblue']\n",
    "                ).update_layout(\n",
    "                    xaxis_title='Date',\n",
    "                    yaxis_title='Average Rating'\n",
    "                )\n",
    "            )\n",
    "        ]),\n",
    "        dcc.Tab(label='Average Rating by Feedback Channel', children=[\n",
    "            dcc.Graph(\n",
    "                id='channel-rating-bar',\n",
    "                figure=px.bar(\n",
    "                    avg_rating_per_channel_pd,\n",
    "                    x='Feedback_Channel',\n",
    "                    y='Average_Rating',\n",
    "                    title=\"Average Rating by Feedback Channel\",\n",
    "                    color='Feedback_Channel',\n",
    "                    color_discrete_sequence=px.colors.qualitative.Vivid\n",
    "                ).update_layout(\n",
    "                    xaxis_title='Feedback Channel',\n",
    "                    yaxis_title='Average Rating'\n",
    "                )\n",
    "            )\n",
    "        ]),\n",
    "\n",
    "        dcc.Tab(label='Rating Distribution Histogram', children=[\n",
    "            dcc.Graph(\n",
    "                id='rating-distribution-hist',\n",
    "                figure=px.histogram(\n",
    "                    pandas_df,\n",
    "                    x='Rating',\n",
    "                    nbins=5,\n",
    "                    title=\"Rating Distribution\",\n",
    "                    color_discrete_sequence=['darkcyan']\n",
    "                ).update_layout(\n",
    "                    xaxis_title='Rating',\n",
    "                    yaxis_title='Frequency'\n",
    "                )\n",
    "            )\n",
    "        ]),\n",
    "    ]),\n",
    "])\n",
    "\n",
    "# 7. Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n",
    "\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
